library(e1071)
library(mlbench)
library(dplyr)
library(class)

# Q1) 
data("PimaIndiansDiabetes", package="mlbench")
pid = PimaIndiansDiabetes[complete.cases(PimaIndiansDiabetes),]
head(pid)
summary(pid)
str(pid)
pid = pid %>% mutate(y=factor(ifelse(diabetes=="pos", 1,0))) %>%
  select(1:8, y)
head(pid)
summary(pid)
str(pid)


# Q2)
set.seed(100)
training.idx = sample(1: nrow(pid), size=nrow(pid)*0.8)
train.data = pid[training.idx,]
test.data = pid[-training.idx,]


# Q3)
# logistic regression
mlogit = glm(y~., data = train.data, family = "binomial")
summary(mlogit)
# significant predictors: pregnant, glucose, pressure, mass, pedigree
# all these predictors have level below 0.05
pred.p = predict(mlogit, newdata=test.data, type="response")
pred.p
y_pred_num = ifelse(pred.p > 0.5, 1, 0)
y_pred = factor(y_pred_num, levels=c(0,1))
# accuracy for the classification
mean(y_pred == test.data$y)
# 0.7597403 accuracy
table(y_pred,test.data$y)


# knn classificaiton
nor = function(x) {
  (x-min(x))/(max(x)-min(x))
}

set.seed(100)
knnpid = pid
knnpid[,1:8] = sapply(knnpid[,1:8], nor)
knnidx = sample(1: nrow(knnpid), size=nrow(knnpid)*0.8)
knntrain.data = knnpid[knnidx,]
knntest.data = knnpid[-knnidx,]
head(knntrain.data)
head(train.data)

ac = rep(0,30)
for (i in 1:30){
  set.seed(100)
  knn.i = knn(knntrain.data[,1:8], knntest.data[,1:8], cl=knntrain.data$y, k=i)
  ac[i] = mean(knn.i == knntest.data$y)
  cat("k=", i, " accuracy=", ac[i], "\n")
}
plot(ac, type="b", xlab="k", ylab="Accuracy")
# best k = 9
set.seed(100)
knn1 = knn(knntrain.data[,1:8], knntest.data[,1:8], cl=knntrain.data$y, k=9)
mean(knn1 == knntest.data$y)
# 0.7727273 accuracy
table(knn1, knntest.data$y)


# svm linear kernel function
m.svm = svm(y~., data = train.data, kernel="linear")
summary(m.svm)
pred.svm = predict(m.svm, newdata=test.data)
table(pred.svm, test.data$y)
mean(pred.svm == test.data$y)
# 0.7597403 accuracy


# svm nonlinear kernel function
m.svm.tune = tune.svm(y~., data=train.data, kernel="radial",
                      cost=10^(-1:2), gamma=c(.1,.5,1,2)) 
summary(m.svm.tune)
plot(m.svm.tune)
best.svm = m.svm.tune$best.model
pred.svm.tune = predict(best.svm, newdata=test.data)
table(pred.svm.tune, test.data$y)
mean(pred.svm.tune==test.data$y)
# 0.7662338 accuracy


# Q4)  
# Logistic Regression (LR):
# Accuracy: 0.7597403
# Misclassification rate: 1 - 0.7597403 = 0.2402597

# k-Nearest Neighbors (kNN):
#   Accuracy: 0.7727273
# Misclassification rate: 1 - 0.7727273 = 0.2272727

# Support Vector Machine (SVM) with non-linear kernel:
#   Accuracy: 0.7662338
# Misclassification rate: 1 - 0.7662338 = 0.2337662

# Based on the comparison of the classification accuracies, the KNN classifier with k=9 has the highest accuracy (0.7727273), 
# followed by the SVM model with non linear kernel (0.7662338) and finally the logistic regression model (0.7597403) 
# In terms of misclassification rates, it follows the same trend

# Logistic Regression (LR):
#  False Positives (FP): 11
#  False Negatives (FN): 26
# k-Nearest Neighbors (kNN):
#  False Positives (FP): 8
#  False Negatives (FN): 27
# Support Vector Machine (SVM) with non-linear kernel:
#  False Positives (FP): 7
#  False Negatives (FN): 29

# The SVM classifier has the lowest number of false positives (7) but highest false negatives (29), which indicates that it is worst at 
# identifying positive cases correctly but best in identifying negative cases correctly compared to the other classifiers. 
# The KNN model has similar false positives (8) with lower false negatives (27) than KNN which indicates that it is better at identifying
# positive cases correctly and roughly similar performance in identifying negative cases correctly
# Finally, the logistic regression has the highest false positives (11) but lowest false negatives (26), which indicates it is best at identifying
# positive cases correctly but worst at identifying negative cases correctly

# In conclusion, the KNN model with k=9 (best choice) seems to provide a good balance between false positives 
# and false negatives while maintaining the highest overall accuracy and lowest misclassification rates


