library(dplyr)
library(class)

nor = function(x) {
  (x -min(x))/(max(x)-min(x))
}

ga = read.csv("/Users/adrian/Library/Mobile Documents/iCloud~md~obsidian/Documents/Notes/2. Reference Notes/R/ps0002/w8/gradadmit.csv", header = T)
head(ga)
str(ga)
ga = ga %>% mutate(y=factor(ifelse(admit==1, 1,0))) %>% 
  select(gre:y)
head(ga)
ga[,1:3] = sapply(ga[,1:3], nor)
head(ga)
set.seed(100)
training.idx = sample(1: nrow(ga), size = nrow(ga)*0.8)
train.data = ga[training.idx, ]
test.data = ga[-training.idx, ]

# knn classificaiton
ac = rep(0,30)
for (i in 1:30){
  set.seed(101)
  knn.i = knn(train.data[,1:3], test.data[,1:3], cl=train.data$y, k=i)
  ac[i] = mean(knn.i == test.data$y)
  cat("k=", i, " accuracy=", ac[i], "\n")
}
plot(ac, type="b", xlab="k", ylab="Accuracy")
# k = 1

set.seed(101)
knn1 = knn(train.data[,1:3], test.data[,1:3], cl=train.data$y, k=1)
mean(knn1 == test.data$y)
table(knn1, test.data$y)

# logistic regression
mlogit = glm(y~., data = train.data, family = "binomial")
summary(mlogit)

pred.p = predict(mlogit, newdata=test.data, type="response")
pred.p

y_pred_num = ifelse(pred.p > 0.5, 1, 0)
y_pred = factor(y_pred_num, levels=c(0,1))

# accuracy for the classification
mean(y_pred == test.data$y)
tab = table(y_pred,test.data$y)
tab
