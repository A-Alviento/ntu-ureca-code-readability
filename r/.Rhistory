pwd
head(ga)
library(dplyr)
library(class)
nor = function(x) {
(x-min(x))/max(x)-min(x)
}
ga = read.csv("/Users/adrian/Library/Mobile Documents/iCloud~md~obsidian/Documents/Notes/2. Reference Notes/R/ps0002/w8/gradadmit.csv", header = T)
head(ga)
ga = ga %>% mutate(y=factor(ifelse(admit==1, 1,0))) %>%
select(gre:y)
head(ga)
ga[,1:3] = sapply(ga[,1:3], nor)
head(ga)
library(dplyr)
library(class)
nor = function(x) {
(x-min(x))/max(x)-min(x)
}
ga = read.csv("/Users/adrian/Library/Mobile Documents/iCloud~md~obsidian/Documents/Notes/2. Reference Notes/R/ps0002/w8/gradadmit.csv", header = T)
head(ga)
ga = ga %>% mutate(y=factor(ifelse(admit==1, 1,0))) %>%
select(gre:y)
head(ga)
ga[,1:3] = sapply(ga[,1:3], nor)
set.seed(100)
training.idx = sample(1: nrow(ga), size = nrow(ga)*0.8)
train.data = ga[training.idx, ]
test.data = ga[-training.idx, ]
# knn classificaiton
ac = rep(0,30)
for (i in 1:30){
set.seed(101)
knn.i = knn(train.data[,1:3], test.data[,1:3], cl=train.data$y, k=i)
ac[i] = mean(knn.i == test.data$y)
cat("k=", i, " accuracy=", ac[i], "\n")
}
plot(ac, type="b", xlab="k", ylab="Accuracy")
head(bc)
library(dplyr)
library(class)
library(mlbench)
data("BreastCancer", package = "mlbench")
# remove NA
bc = BreastCancer[complete.cases(BreastCancer),]
head(bc)
bc[,2:4] = sapply(bc[,2:4], as.numeric)
head(bc)
bc<-bc%>%mutate(y=factor(ifelse(Class=="malignant", 1,0)))%>%select(Cl.thickness:Cell.shape, y)
# normalise numeric variables
nor = function(x) {
(x-min(x))/max(x)-min(x)
}
bc[,1:3] = sapply(bc[,1:3], nor)
head(bc)
library(e1071)
library(mlbench)
library(dplyr)
library(class)
# Q1)
data("PimaIndiansDiabetes", package="mlbench")
pid = PimaIndiansDiabetes[complete.cases(PimaIndiansDiabetes),]
head(pid)
summary(pid)
str(pid)
pid = pid %>% mutate(y=factor(ifelse(diabetes=="pos", 1,0))) %>%
select(1:8, y)
head(pid)
summary(pid)
str(pid)
# Q2)
set.seed(100)
training.idx = sample(1: nrow(pid), size=nrow(pid)*0.8)
train.data = pid[training.idx,]
test.data = pid[-training.idx,]
# Q3)
# logistic regression
mlogit = glm(y~., data = train.data, family = "binomial")
summary(mlogit)
library(foreign)
library(dplyr)
# read in dataset from Scalabrino
featuresDF = read.arff("features.arff.txt")
colnames(featuresDF)
# read in scores of the 200 snippets
scoresDF = read.csv("scores.csv")
scoresDF
colnames(scoresDF)
# transpose the dataframe so the columns are the evaluator scores
# and the rows are the snippets
scoresDF = t(scoresDF)
# make the first row the column heading
colnames(scoresDF) = scoresDF[1, ]
colnames(scoresDF)
# remove first row since it's just the colnames
scoresDF = scoresDF[-1, ]
# convert entries from chr to numeric
scoresDF = apply(scoresDF, 2, as.numeric)
# turn matrix into df
scoresDF = as.data.frame(scoresDF)
# add mean score column
scoresDF = scoresDF %>% mutate(meanScore = (Evaluator1 + Evaluator2 + Evaluator3 +
Evaluator4 + Evaluator5 + Evaluator6 +
Evaluator7 + Evaluator8 + Evaluator9)/9 )
head(scoresDF)
# Plot density and add mean line
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
legend("topright", legend = c("Density", "Mean"), col = c("black", "red"), lty = c(1, 2), cex = 0.8)
library(foreign)
library(dplyr)
# read in dataset from Scalabrino
featuresDF = read.arff("features.arff.txt")
colnames(featuresDF)
# read in scores of the 200 snippets
scoresDF = read.csv("scores.csv")
scoresDF
colnames(scoresDF)
# transpose the dataframe so the columns are the evaluator scores
# and the rows are the snippets
scoresDF = t(scoresDF)
# make the first row the column heading
colnames(scoresDF) = scoresDF[1, ]
colnames(scoresDF)
# remove first row since it's just the colnames
scoresDF = scoresDF[-1, ]
# convert entries from chr to numeric
scoresDF = apply(scoresDF, 2, as.numeric)
# turn matrix into df
scoresDF = as.data.frame(scoresDF)
# add mean score column
scoresDF = scoresDF %>% mutate(meanScore = (Evaluator1 + Evaluator2 + Evaluator3 +
Evaluator4 + Evaluator5 + Evaluator6 +
Evaluator7 + Evaluator8 + Evaluator9)/9 )
head(scoresDF)
# Plot density and add mean line
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
# Plot density and add mean line
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
legend("topright", legend = c("Density", "Mean"), col = c("black", "red"), lty = c(1, 2), cex = 0.8)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
abline(v = meanTotalScore, col = "red", lty = 2)
# Plot density and add mean line
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
# Plot density and add mean line
meanTotalScore <- mean(scoresDF$meanScore)
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
meanTotalScore
# Plot density and add mean line
meanTotalScore <- mean(scoresDF$meanScore)
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
legend("topright", legend = c("Density", "Mean"), col = c("black", "red"), lty = c(1, 2), cex = 0.8)
# read in own features implementation
ownFeaturesDF = read.csv("feature_matrix_5.csv", header = FALSE)
head(ownFeaturesDF)
# name column header
colnames(ownFeaturesDF) = c("numLines", "numBlankLines", "numChars", "ratioBlank", "avgCharsLine",
"halsteadVol", "numComment", "ratioComment", "commentReadabilityGF",
"commentReadabilityFK", "numIdentifier", "numEngIdentifier", "numNewIdentifier",
"numNewEngIdentifier", "avgNumId", "ratioEngIdOverId", "avgNewId",
"ratioNewEngIdOverNewId", "numMeaningfulId", "ratioMeaningufulId",
"avgIdLen", "numIndentBlocks", "ratioIndentNumLines", "maxIndent",
"numCommentBlk", "ratioCommentBlock")
# extract the readable column into another dataframe
isReadable = featuresDF["Readable"]
# add both isReadable into ownFeaturesDF
ownFeaturesDF_isReadable = cbind(ownFeaturesDF, isReadable)
# copy dataset to ds, the dataset to test the subsets
ds = ownFeaturesDF_isReadable
# generate a heatmap of the correlation coefficient of each feature variables
library(corrplot)
matrix_plot = apply(ownFeaturesDF_isReadable, 2, as.numeric)
matrix_plot
str(matrix_plot)
corrplot(cor(matrix_plot),type="upper",method="color",addCoef.col = "black",number.cex = 0.6)
head(ds, 27)
# generate list of pairs of feature variables that are highly correlated
# turn the matrix of features into a dataframe
matrix_plot = as.data.frame(matrix_plot)
colnames(matrix_plot) = c("numLines", "numBlankLines", "numChars", "ratioBlank", "avgCharsLine",
"halsteadVol", "numComment", "ratioComment", "commentReadabilityGF",
"commentReadabilityFK", "numIdentifier", "numEngIdentifier", "numNewIdentifier",
"numNewEngIdentifier", "avgNumId", "ratioEngIdOverId", "avgNewId",
"ratioNewEngIdOverNewId", "numMeaningfulId", "ratioMeaningufulId",
"avgIdLen", "numIndentBlocks", "ratioIndentNumLines", "maxIndent",
"numCommentBlk", "ratioCommentBlock", "Readable")
# Create a list of all pairs of variables
var_pairs <- combn(names(matrix_plot), 2, simplify = FALSE)
# Calculate the correlation coefficient for each pair of variables
cor_vals <- sapply(var_pairs, function(x) cor(matrix_plot[[x[1]]], matrix_plot[[x[2]]]))
# Create a data frame with the variable pairs and their correlation coefficients
cor_df <- data.frame(Var1 = sapply(var_pairs, "[[", 1), Var2 = sapply(var_pairs, "[[", 2), Corr = cor_vals)
# Filter the data frame to show only highly correlated pairs (absolute correlation coefficient > 0.7)
high_corr_pairs <- subset(cor_df, abs(Corr) > 0.7)
# Print the highly correlated pairs
print(high_corr_pairs)
# Create a data frame
df <- data.frame(
name = c("Alice", "Bob", "Charlie"),
age = c(25, 30, 35),
gender = c("female", "male", "male")
)
df
# Add a new row to the data frame
new_row <- c("Dave", 40, "male")
df <- rbind(df, new_row)
# Print the updated data frame
print(df)
y = as.data.frame(y)
# variable to hold accuracy and F1 value
y = c(0, 0, 0)
y = as.data.frame(y)
y
# dataframe to hold all accuracy, F1 and AUC of each seed
x = data.frame(
Accuracy
F1
# dataframe to hold all accuracy, F1 and AUC of each seed
x = data.frame(
Accuracy
F1
# dataframe to hold all accuracy, F1 and AUC of each seed
x = data.frame(
Accuracy = c()
F1 = c()
# dataframe to hold all accuracy, F1 and AUC of each seed
x <- data.frame()
colnames(x) <- c("Accuracy", "F1", "AUC")
colnames(x) <- c("Accuracy", "F1", "AUC")
x
# dataframe to hold all accuracy, F1 and AUC of each seed
x <- data.frame()
colnames(x) <- c("Accuracy", "F1", "AUC")
# Add a new row to the data frame
x <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
# Print the updated data frame
print(df)
# dataframe to hold all accuracy, F1 and AUC of each seed
x <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
x
# Add a new row to the data frame
z <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
# dataframe to hold all accuracy, F1 and AUC of each seed
z <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
z
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
z
# dataframe to hold all accuracy, F1 and AUC of each seed
z <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
z
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
# variable to hold accuracy, F1 value and AUC
y = c(0, 0, 0)
y = as.data.frame(y)
# dataframe to hold all accuracy, F1 and AUC of each seed
z <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
z
# multiple seeded runs
for(k in 1:10){
y = y + f2(k, z)
new_row <- f2(k,z)
z <- rbind(z, new_row)
}
z
# variable to hold accuracy, F1 value and AUC
y = c(0, 0, 0)
y = as.data.frame(y)
# dataframe to hold all accuracy, F1 and AUC of each seed
z <- data.frame(Accuracy = numeric(), F1 = numeric(), AUC = numeric())
z = rbind(z, f2(k,z))
z
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
colnames(z) = c("Accuracy", "F1", "AUC")
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
source("~/Desktop/cs-projects/ureca-code-readability-new/r/test_F1_Accuracy.R", echo=TRUE)
ggplot(z, aes(x = "Accuracy", y = Accuracy)) +
geom_boxplot() + labs(x = "Seeded Runs")
ggplot(z, aes(x = "Accuracy", y = Accuracy)) +
geom_boxplot() + labs(x = "Seeded Runs")
ggplot(z, aes(x = "Accuracy", y = Accuracy)) +
geom_boxplot() + labs(x = "Seeded Runs")
head(scoresDF)
library(foreign)
library(dplyr)
# read in dataset from Scalabrino
featuresDF = read.arff("features.arff.txt")
colnames(featuresDF)
# read in scores of the 200 snippets
scoresDF = read.csv("scores.csv")
scoresDF
colnames(scoresDF)
# transpose the dataframe so the columns are the evaluator scores
# and the rows are the snippets
scoresDF = t(scoresDF)
# make the first row the column heading
colnames(scoresDF) = scoresDF[1, ]
colnames(scoresDF)
# remove first row since it's just the colnames
scoresDF = scoresDF[-1, ]
# convert entries from chr to numeric
scoresDF = apply(scoresDF, 2, as.numeric)
# turn matrix into df
scoresDF = as.data.frame(scoresDF)
# add mean score column
scoresDF = scoresDF %>% mutate(meanScore = (Evaluator1 + Evaluator2 + Evaluator3 +
Evaluator4 + Evaluator5 + Evaluator6 +
Evaluator7 + Evaluator8 + Evaluator9)/9 )
head(scoresDF)
# Plot density and add mean line
meanTotalScore <- mean(scoresDF$meanScore)
plot(density(scoresDF$meanScore), col = "black", lwd = 2, main = "Density Plot of Last Column")
abline(v = meanTotalScore, col = "red", lty = 2)
text(meanTotalScore, max(density(scoresDF$meanScore)$y), paste0("Mean = ", round(meanTotalScore, 2)), pos = 2)
legend("topright", legend = c("Density", "Mean"), col = c("black", "red"), lty = c(1, 2), cex = 0.8)
# read in own features implementation
ownFeaturesDF = read.csv("feature_matrix_5.csv", header = FALSE)
head(ownFeaturesDF)
# name column header
colnames(ownFeaturesDF) = c("numLines", "numBlankLines", "numChars", "ratioBlank", "avgCharsLine",
"halsteadVol", "numComment", "ratioComment", "commentReadabilityGF",
"commentReadabilityFK", "numIdentifier", "numEngIdentifier", "numNewIdentifier",
"numNewEngIdentifier", "avgNumId", "ratioEngIdOverId", "avgNewId",
"ratioNewEngIdOverNewId", "numMeaningfulId", "ratioMeaningufulId",
"avgIdLen", "numIndentBlocks", "ratioIndentNumLines", "maxIndent",
"numCommentBlk", "ratioCommentBlock")
# extract the readable column into another dataframe
isReadable = featuresDF["Readable"]
# add both isReadable into ownFeaturesDF
ownFeaturesDF_isReadable = cbind(ownFeaturesDF, isReadable)
# copy dataset to ds, the dataset to test the subsets
ds = ownFeaturesDF_isReadable
# generate a heatmap of the correlation coefficient of each feature variables
library(corrplot)
matrix_plot = apply(ownFeaturesDF_isReadable, 2, as.numeric)
matrix_plot
str(matrix_plot)
corrplot(cor(matrix_plot),type="upper",method="color",addCoef.col = "black",number.cex = 0.6)
head(ds, 27)
# generate list of pairs of feature variables that are highly correlated
# turn the matrix of features into a dataframe
matrix_plot = as.data.frame(matrix_plot)
colnames(matrix_plot) = c("numLines", "numBlankLines", "numChars", "ratioBlank", "avgCharsLine",
"halsteadVol", "numComment", "ratioComment", "commentReadabilityGF",
"commentReadabilityFK", "numIdentifier", "numEngIdentifier", "numNewIdentifier",
"numNewEngIdentifier", "avgNumId", "ratioEngIdOverId", "avgNewId",
"ratioNewEngIdOverNewId", "numMeaningfulId", "ratioMeaningufulId",
"avgIdLen", "numIndentBlocks", "ratioIndentNumLines", "maxIndent",
"numCommentBlk", "ratioCommentBlock", "Readable")
# Create a list of all pairs of variables
var_pairs <- combn(names(matrix_plot), 2, simplify = FALSE)
# Calculate the correlation coefficient for each pair of variables
cor_vals <- sapply(var_pairs, function(x) cor(matrix_plot[[x[1]]], matrix_plot[[x[2]]]))
# Create a data frame with the variable pairs and their correlation coefficients
cor_df <- data.frame(Var1 = sapply(var_pairs, "[[", 1), Var2 = sapply(var_pairs, "[[", 2), Corr = cor_vals)
# Filter the data frame to show only highly correlated pairs (absolute correlation coefficient > 0.7)
high_corr_pairs <- subset(cor_df, abs(Corr) > 0.7)
# Print the highly correlated pairs
print(high_corr_pairs)
ds
gradAdmit = read.csv("gradadmit.csv", T)
set.seed(100)
# split dataset into training and testing sets
training.idx = sample(1: nrow(ownFeaturesDF_isReadable), nrow(ownFeaturesDF_isReadable)*0.8)
train.data = ownFeaturesDF_isReadable[training.idx, ]
test.data = ownFeaturesDF_isReadable[-training.idx, ]
# Training final model with selected features
final_model <- train(Readable~numLines + avgCharsLine + ratioComment + ratioBlank +
avgNewId + avgIdLen + ratioIndentNumLines + commentReadabilityFK,
data = train.data, trControl = trainControl(method = "cv", number = 10),
method = "glm", family = binomial())
final_model
summary(final_model)
library(glmnet)
library(caret)
library(pROC)
library(ROCR)
library("MLmetrics")
library(car)
library(FSelector)
# renew values for the dataset
ownFeaturesDF_isReadable = ds
ds
# Loop through identified columns and remove outliers from active features being used
for (col in colnames(ownFeaturesDF_isReadable[,c("numLines", "avgCharsLine","ratioComment", "ratioBlank",
"avgNewId", "avgIdLen", "ratioIndentNumLines", "commentReadabilityFK")])) {
q <- quantile(ownFeaturesDF_isReadable[[col]], c(0.25, 0.75), na.rm=TRUE)
iqr <- q[2] - q[1]
lower_bound <- q[1] - 1.5 * iqr
upper_bound <- q[2] + 1.5 * iqr
ownFeaturesDF_isReadable[[col]][ownFeaturesDF_isReadable[[col]] < lower_bound | ownFeaturesDF_isReadable[[col]] > upper_bound] <- NA
}
ownFeaturesDF_isReadable = na.omit(ownFeaturesDF_isReadable)
set.seed(100)
# split dataset into training and testing sets
training.idx = sample(1: nrow(ownFeaturesDF_isReadable), nrow(ownFeaturesDF_isReadable)*0.8)
train.data = ownFeaturesDF_isReadable[training.idx, ]
test.data = ownFeaturesDF_isReadable[-training.idx, ]
# Training final model with selected features
final_model <- train(Readable~numLines + avgCharsLine + ratioComment + ratioBlank +
avgNewId + avgIdLen + ratioIndentNumLines + commentReadabilityFK,
data = train.data, trControl = trainControl(method = "cv", number = 10),
method = "glm", family = binomial())
final_model
summary(final_model)
# Make predictions on test set
predictions <- predict(final_model, newdata = test.data, type = "raw")
# Calculate performance metrics
confusion_mat <- confusionMatrix(predictions, test.data$Readable, mode = "everything", positive="1")
roc_obj <- roc(as.numeric(test.data$Readable)-1, as.numeric(predictions), levels = c("0", "1"))
auc <- auc(roc_obj)
confusion_mat$overall['Accuracy']
confusion_mat$byClass['F1']
set.seed(101)
# split dataset into training and testing sets
training.idx = sample(1: nrow(ownFeaturesDF_isReadable), nrow(ownFeaturesDF_isReadable)*0.8)
train.data = ownFeaturesDF_isReadable[training.idx, ]
test.data = ownFeaturesDF_isReadable[-training.idx, ]
# Training final model with selected features
final_model <- train(Readable~numLines + avgCharsLine + ratioComment + ratioBlank +
avgNewId + avgIdLen + ratioIndentNumLines + commentReadabilityFK,
data = train.data, trControl = trainControl(method = "cv", number = 10),
method = "glm", family = binomial())
final_model
summary(final_model)
# Make predictions on test set
predictions <- predict(final_model, newdata = test.data, type = "raw")
# Calculate performance metrics
confusion_mat <- confusionMatrix(predictions, test.data$Readable, mode = "everything", positive="1")
roc_obj <- roc(as.numeric(test.data$Readable)-1, as.numeric(predictions), levels = c("0", "1"))
auc <- auc(roc_obj)
confusion_mat$overall['Accuracy']
confusion_mat$byClass['F1']
summary(final_model)
